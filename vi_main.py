# app.py
import streamlit as st
from streamlit_chat import message
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.text_splitter import TokenTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain_community.document_loaders import PyPDFLoader
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from dotenv import load_dotenv
import os
import tempfile
import logging

# CÃ i Ä‘áº·t logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
load_dotenv()

def init_session_state():
    st.session_state.setdefault('history', [])
    st.session_state.setdefault('generated', ["CÃ¡c pháº£n há»“i cá»§a báº¡n sáº½ Ä‘Æ°á»£c hiá»ƒn thá»‹ á»Ÿ Ä‘Ã¢y."])
    st.session_state.setdefault('past', ["Xin chÃ o! ğŸ‘‹ HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» tÃ i liá»‡u PDF ğŸ“„"])

def reset_chat():
    st.session_state['history'] = []
    st.session_state['past'] = ["Xin chÃ o! ğŸ‘‹ HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» tÃ i liá»‡u PDF ğŸ“„"]
    st.session_state.pop('chain', None)
    st.session_state.pop('vector_store', None)

def conversation_chat(query, chain, history):
    try:
        result = chain.invoke({"question": query, "chat_history": history})
        answer = result["answer"].strip()
        history.append((query, answer))
        return answer
    except Exception as e:
        logger.error(f"Lá»—i há»™i thoáº¡i: {e}")
        error_msg = f"Xin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ yÃªu cáº§u nÃ y ngay bÃ¢y giá». Lá»—i: {e}"
        history.append((query, error_msg))
        return error_msg

def display_chat(chain):
    with st.form("chat_form", clear_on_submit=True):
        user_input = st.text_input("CÃ¢u há»i:", placeholder="HÃ£y há»i vá» ná»™i dung cá»§a tÃ i liá»‡u PDF", key='input')
        if st.form_submit_button("Gá»­i") and user_input:
            with st.spinner("Äang táº¡o pháº£n há»“i..."):
                output = conversation_chat(user_input, chain, st.session_state['history'])
                st.session_state['past'].append(user_input)
                st.session_state['generated'].append(output)

    for i in reversed(range(len(st.session_state['generated']))):
        if i < len(st.session_state["past"]):
            message(st.session_state["past"][i], is_user=True, key=f"{i}_user")
        message(st.session_state["generated"][i], key=str(i))

def create_chain(vector_store):
    llm = AzureChatOpenAI(
        deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"),
        model_name=os.getenv("AZURE_OPENAI_CHAT_MODEL_NAME"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        openai_api_version=os.getenv("OPENAI_API_CHAT_MODEL_VERSION"),
        openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        openai_api_type="azure"
    )

    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 2})

    template = """HÃ£y sá»­ dá»¥ng ngá»¯ cáº£nh sau Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i. Tráº£ lá»i **chá»‰ báº±ng tiáº¿ng Viá»‡t**:
Ngá»¯ cáº£nh:
{context}

CÃ¢u há»i: {question}

Tráº£ lá»i:"""
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        combine_docs_chain_kwargs={'prompt': prompt},
        return_source_documents=False
    )

def process_pdfs(files):
    documents = []
    for f in files:
        temp_file_path = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                tmp.write(f.read())
                temp_file_path = tmp.name

            loader = PyPDFLoader(temp_file_path)
            docs = loader.load()
            documents.extend(docs)
            logger.info(f"ÄÃ£ táº£i {len(docs)} trang tá»« {f.name}")

        except Exception as e:
            logger.error(f"Lá»—i khi táº£i PDF {f.name}: {e}")
            st.error(f"KhÃ´ng thá»ƒ táº£i PDF: {f.name}. Vui lÃ²ng kiá»ƒm tra Ä‘á»‹nh dáº¡ng tá»‡p.")
        finally:
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                    logger.info(f"ÄÃ£ xÃ³a tá»‡p táº¡m thá»i: {temp_file_path}")
                except OSError as e:
                    logger.error(f"Lá»—i khi xÃ³a tá»‡p táº¡m thá»i {temp_file_path}: {e}")

    if not documents:
        st.error("KhÃ´ng tÃ i liá»‡u nÃ o Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng tá»« cÃ¡c tá»‡p PDF.")
        return None

    splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=40)
    chunks = splitter.split_documents(documents)
    logger.info(f"ÄÃ£ chia tÃ i liá»‡u thÃ nh {len(chunks)} Ä‘oáº¡n")

    try:
        embeddings = AzureOpenAIEmbeddings(
            deployment=os.getenv("OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME"),
            model=os.getenv("OPENAI_ADA_EMBEDDING_MODEL_NAME"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_type="azure",
            chunk_size=100
        )
        logger.info("ÄÃ£ khá»Ÿi táº¡o AzureOpenAIEmbeddings thÃ nh cÃ´ng.")
    except Exception as e:
        logger.error(f"Lá»—i khi khá»Ÿi táº¡o AzureOpenAIEmbeddings: {e}")
        st.error("KhÃ´ng thá»ƒ khá»Ÿi táº¡o Embeddings tá»« Azure OpenAI. Vui lÃ²ng kiá»ƒm tra biáº¿n mÃ´i trÆ°á»ng.")
        st.error(f"Lá»—i cá»¥ thá»ƒ: {e}")
        return None

    try:
        logger.info("Äang táº¡o Chroma vector store...")
        vector_store = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            collection_name="pdf_collection",
            persist_directory="./chroma_db"
        )
        logger.info("ÄÃ£ táº¡o vÃ  lÆ°u trá»¯ Chroma vector store.")
        return vector_store
    except Exception as e:
        logger.error(f"Lá»—i khi táº¡o vector store: {e}")
        st.error("KhÃ´ng thá»ƒ táº¡o vector store. Vui lÃ²ng kiá»ƒm tra cáº¥u hÃ¬nh embeddings vÃ  lÆ°u trá»¯.")
        st.error(f"Lá»—i cá»¥ thá»ƒ: {e}")
        return None

def main():
    st.set_page_config(page_title="TrÃ² chuyá»‡n vá»›i PDF", page_icon="ğŸ“š")
    st.title("ğŸ“˜ TrÃ² chuyá»‡n vá»›i tÃ i liá»‡u báº¡n Ä‘Ã£ táº£i lÃªn.")
    init_session_state()

    st.sidebar.header("Táº£i lÃªn & TÃ¹y chá»n")
    uploaded_files = st.sidebar.file_uploader("Táº£i lÃªn tá»‡p PDF", type="pdf", accept_multiple_files=True)

    if st.sidebar.button("ğŸ”„ Äáº·t láº¡i TrÃ² chuyá»‡n"):
        reset_chat()
        st.rerun()

    process_button = st.sidebar.button("ğŸš€ Xá»­ lÃ½ tá»‡p Ä‘Ã£ táº£i lÃªn")

    if uploaded_files and (process_button or "chain" not in st.session_state or st.session_state['chain'] is None):
        st.session_state.pop('chain', None)
        st.session_state.pop('vector_store', None)
        reset_chat()

        with st.spinner("Äang xá»­ lÃ½ tÃ i liá»‡u... QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt Ä‘á»‘i vá»›i cÃ¡c tá»‡p lá»›n."):
            vector_store = process_pdfs(uploaded_files)
            if vector_store:
                st.session_state['vector_store'] = vector_store
                st.session_state['chain'] = create_chain(vector_store)
                st.success("TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng! BÃ¢y giá» báº¡n cÃ³ thá»ƒ Ä‘áº·t cÃ¢u há»i.")

    if "chain" in st.session_state and st.session_state['chain'] is not None:
        display_chat(st.session_state["chain"])
    else:
        st.info("Vui lÃ²ng táº£i lÃªn tÃ i liá»‡u PDF vÃ  nháº¥n 'Xá»­ lÃ½ tá»‡p Ä‘Ã£ táº£i lÃªn' Ä‘á»ƒ báº¯t Ä‘áº§u.")

if __name__ == "__main__":
    main()
