import streamlit as st
from streamlit_chat import message
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings # Assuming correct import paths
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document # Needed to create Document objects for lines
from dotenv import load_dotenv
import os
import logging
import tempfile
import shutil # Import shutil for directory removal
import re # Import regex for checking image lines

# Assuming process_pdf.py is in the same directory or accessible
try:
    from process_pdf import get_md_text
    logger = logging.getLogger(__name__)
    logger.info("ƒê√£ nh·∫≠p h√†m get_md_text t·ª´ process_pdf.py th√†nh c√¥ng.")
except ImportError:
    logger = logging.getLogger(__name__)
    logger.error("Kh√¥ng t√¨m th·∫•y process_pdf.py ho·∫∑c h√†m get_md_text. Vui l√≤ng ƒë·∫£m b·∫£o t·ªáp t·ªìn t·∫°i.")
    st.error("L·ªói: Kh√¥ng t√¨m th·∫•y t·ªáp process_pdf.py ho·∫∑c h√†m get_md_text. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t.")
    # Define a dummy function to prevent errors if import fails, though the app won't work correctly
    def get_md_text(pdf_path, output_dir_base="md_output", poppler_path=None):
        logger.error("H√†m get_md_text kh√¥ng kh·∫£ d·ª•ng do l·ªói nh·∫≠p.")
        return ""


# C√†i ƒë·∫∑t logging
logging.basicConfig(level=logging.INFO)
load_dotenv()

# Define the fixed markdown output file name
MARKDOWN_OUTPUT_FILENAME = "document.md"
# Define the directory for markdown output
MARKDOWN_OUTPUT_DIR = "md_output"
# Define the full path to the markdown output file
MARKDOWN_OUTPUT_FILE_PATH = os.path.join(MARKDOWN_OUTPUT_DIR, MARKDOWN_OUTPUT_FILENAME)
# Define the directory for Chroma DB
CHROMA_DB_DIR = "./chroma_db"

# Define the fixed Poppler path
FIXED_POPPLER_PATH = r".\poppler-24.08.0\Library\bin"

# Regex to identify markdown image lines
IMAGE_LINE_REGEX = re.compile(r'^!\[.*?\]\(.*?\)$')


def init_session_state():
    """Initializes Streamlit session state variables."""
    st.session_state.setdefault('history', [])
    st.session_state.setdefault('generated', ["C√°c ph·∫£n h·ªìi c·ªßa b·∫°n s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã ·ªü ƒë√¢y."])
    st.session_state.setdefault('past', ["Xin ch√†o! üëã H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu PDF ƒë√£ t·∫£i l√™n üìÑ"])
    st.session_state.setdefault('processed_pdf_name', None) # To store the name of the processed PDF
    st.session_state.setdefault('vector_store_ready', False) # Flag to indicate if vector store is ready


def reset_chat():
    """Resets the chat history and processing state."""
    st.session_state['history'] = []
    st.session_state['past'] = ["Xin ch√†o! üëã H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu PDF ƒë√£ t·∫£i l√™n üìÑ"]
    st.session_state.pop('chain', None)
    st.session_state.pop('vector_store', None)
    st.session_state['processed_pdf_name'] = None
    st.session_state['vector_store_ready'] = False
    # Clean up generated markdown file and chroma db directory
    if os.path.exists(MARKDOWN_OUTPUT_FILE_PATH):
        try:
            os.remove(MARKDOWN_OUTPUT_FILE_PATH)
            logger.info(f"ƒê√£ x√≥a t·ªáp markdown c≈©: {MARKDOWN_OUTPUT_FILE_PATH}")
        except OSError as e:
            logger.error(f"L·ªói khi x√≥a t·ªáp markdown c≈© {MARKDOWN_OUTPUT_FILE_PATH}: {e}")
    if os.path.exists(MARKDOWN_OUTPUT_DIR):
         try:
             shutil.rmtree(MARKDOWN_OUTPUT_DIR)
             logger.info(f"ƒê√£ x√≥a th∆∞ m·ª•c markdown c≈©: {MARKDOWN_OUTPUT_DIR}")
         except OSError as e:
             logger.error(f"L·ªói khi x√≥a th∆∞ m·ª•c markdown c≈© {MARKDOWN_OUTPUT_DIR}: {e}")
    if os.path.exists(CHROMA_DB_DIR):
        try:
            shutil.rmtree(CHROMA_DB_DIR)
            logger.info(f"ƒê√£ x√≥a th∆∞ m·ª•c Chroma DB c≈©: {CHROMA_DB_DIR}")
        except OSError as e:
            logger.error(f"L·ªói khi x√≥a th∆∞ m·ª•c Chroma DB c≈© {CHROMA_DB_DIR}: {e}")

    logger.info("Chat v√† tr·∫°ng th√°i x·ª≠ l√Ω ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t l·∫°i.")


def conversation_chat(query, chain, history):
    """Handles the conversational chat interaction."""
    try:
        logger.info(f"Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng: {query}")
        result = chain.invoke({"question": query, "chat_history": history})
        answer = result["answer"].strip()
        history.append((query, answer))
        logger.info("ƒê√£ t·∫°o ph·∫£n h·ªìi th√†nh c√¥ng.")
        return answer
    except Exception as e:
        logger.error(f"L·ªói h·ªôi tho·∫°i: {e}")
        error_msg = f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y ngay b√¢y gi·ªù. L·ªói: {e}"
        history.append((query, error_msg))
        return error_msg

def display_chat(chain):
    """Displays the chat interface."""
    with st.form("chat_form", clear_on_submit=True):
        user_input = st.text_input("C√¢u h·ªèi:", placeholder=f"H√£y h·ªèi v·ªÅ n·ªôi dung c·ªßa t√†i li·ªáu {st.session_state.get('processed_pdf_name', 'PDF')}", key='input')
        if st.form_submit_button("G·ª≠i") and user_input:
            with st.spinner("ƒêang t·∫°o ph·∫£n h·ªìi..."):
                output = conversation_chat(user_input, chain, st.session_state['history'])
                st.session_state['past'].append(user_input)
                st.session_state['generated'].append(output)
            # Rerun to update the chat display
            st.rerun()

    # Display chat messages
    if st.session_state['generated']:
        for i in reversed(range(len(st.session_state['generated']))):
            if i < len(st.session_state["past"]):
                message(st.session_state["past"][i], is_user=True, key=f"{i}_user")
            message(st.session_state["generated"][i], key=str(i))

def create_chain(vector_store):
    """Creates a conversational retrieval chain."""
    try:
        logger.info("ƒêang t·∫°o chu·ªói h·ªôi tho·∫°i...")
        llm = AzureChatOpenAI(
            deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"),
            model_name=os.getenv("AZURE_OPENAI_CHAT_MODEL_NAME"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_version=os.getenv("OPENAI_API_CHAT_MODEL_VERSION"),
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            openai_api_type="azure",
            temperature=0 # Set temperature to 0 for more deterministic responses
        )

        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        # Adjust search_kwargs if needed, e.g., number of relevant documents to retrieve
        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

        # Prompt for markdown context
        template = """S·ª≠ d·ª•ng c√°c ƒëo·∫°n vƒÉn b·∫£n sau t·ª´ t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi **ch·ªâ b·∫±ng ti·∫øng Vi·ªát**.
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë t·∫°o ra c√¢u tr·∫£ l·ªùi.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:"""
        prompt = PromptTemplate(template=template, input_variables=["context", "question"])

        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            combine_docs_chain_kwargs={'prompt': prompt},
            return_source_documents=False # Set to True if you want to see source chunks
        )
        logger.info("ƒê√£ t·∫°o chu·ªói h·ªôi tho·∫°i th√†nh c√¥ng.")
        return chain
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o chu·ªói h·ªôi tho·∫°i: {e}")
        st.error("Kh√¥ng th·ªÉ t·∫°o chu·ªói h·ªôi tho·∫°i. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh Azure OpenAI.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None

def process_markdown_file_lines(markdown_file_path: str):
    """
    Reads a markdown file and creates document chunks based on content
    following image lines.
    """
    chunks = []
    current_chunk_lines = []
    if not os.path.exists(markdown_file_path):
        logger.error(f"L·ªói: T·ªáp markdown kh√¥ng t√¨m th·∫•y t·∫°i {markdown_file_path}")
        st.error(f"L·ªói: T·ªáp markdown kh√¥ng t√¨m th·∫•y t·∫°i {markdown_file_path}. Qu√° tr√¨nh chuy·ªÉn ƒë·ªïi PDF sang Markdown c√≥ th·ªÉ ƒë√£ th·∫•t b·∫°i ho·∫∑c t·ªáp kh√¥ng ƒë∆∞·ª£c l∆∞u ƒë√∫ng c√°ch.")
        return None

    try:
        logger.info(f"ƒêang ƒë·ªçc t·ªáp markdown v√† t·∫°o chunks d·ª±a tr√™n h√¨nh ·∫£nh: {markdown_file_path}")
        with open(markdown_file_path, 'r', encoding='utf-8') as f:
            # Read the file line by line
            for i, line in enumerate(f):
                cleaned_line = line.strip()

                # Check if the cleaned line is an image line
                if IMAGE_LINE_REGEX.match(cleaned_line):
                    # If we encounter an image line, the content accumulated so far
                    # (if any) forms a chunk.
                    if current_chunk_lines:
                        chunk_content = "\n".join(current_chunk_lines)
                        # Create a Document object for the chunk
                        # Metadata will reflect the source file and the starting line of the chunk
                        chunks.append(Document(page_content=chunk_content, metadata={"source": os.path.basename(markdown_file_path), "start_line": i - len(current_chunk_lines) + 1}))
                        logger.debug(f"ƒê√£ t·∫°o chunk t·ª´ d√≤ng {i - len(current_chunk_lines) + 1} ƒë·∫øn {i-1}")

                    # Clear the buffer to start accumulating content for the next chunk
                    current_chunk_lines = []
                    # Do NOT add the image line itself to the current chunk buffer,
                    # as the chunk is defined by the content *after* the image.

                elif cleaned_line: # If it's not an image line and not an empty line
                    # Add the cleaned line to the current chunk buffer
                    current_chunk_lines.append(cleaned_line)

                # If it's an empty line and not an image line, it's skipped by the elif condition

        # After the loop, add any remaining content in the buffer as the last chunk
        if current_chunk_lines:
            chunk_content = "\n".join(current_chunk_lines)
            chunks.append(Document(page_content=chunk_content, metadata={"source": os.path.basename(markdown_file_path), "start_line": i - len(current_chunk_lines) + 2})) # Adjust start_line for the last chunk
            logger.debug(f"ƒê√£ t·∫°o chunk cu·ªëi c√πng t·ª´ d√≤ng {i - len(current_chunk_lines) + 2} ƒë·∫øn {i+1}")


        logger.info(f"ƒê√£ t·∫°o {len(chunks)} chunks d·ª±a tr√™n h√¨nh ·∫£nh t·ª´ {markdown_file_path}")
        return chunks

    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc t·ªáp markdown v√† t·∫°o chunks {markdown_file_path}: {e}")
        st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc t·ªáp markdown ho·∫∑c t·∫°o chunks: {markdown_file_path}. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng t·ªáp ho·∫∑c quy·ªÅn truy c·∫≠p.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None


def create_vector_store_from_documents(documents: list[Document]):
    """
    Generates a Chroma vector store using Azure OpenAI embeddings from a list of documents.
    """
    if not documents:
        st.error("Kh√¥ng c√≥ t√†i li·ªáu n√†o ƒë·ªÉ t·∫°o vector store.")
        return None

    try:
        logger.info("ƒêang kh·ªüi t·∫°o AzureOpenAIEmbeddings...")
        # Ensure these environment variables are set
        embeddings = AzureOpenAIEmbeddings(
            deployment=os.getenv("OPENAI_ADA_EMBEDDING_EMBEDDING_DEPLOYMENT_NAME"), # Corrected env var name
            model=os.getenv("OPENAI_ADA_EMBEDDING_MODEL_NAME"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_type="azure",
            chunk_size=100 # This chunk_size is for the embedding model's internal batching
        )
        logger.info("ƒê√£ kh·ªüi t·∫°o AzureOpenAIEmbeddings th√†nh c√¥ng.")
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o AzureOpenAIEmbeddings: {e}")
        st.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Embeddings t·ª´ Azure OpenAI. Vui l√≤ng ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None

    try:
        logger.info(f"ƒêang t·∫°o Chroma vector store t·ª´ c√°c d√≤ng v√†o {CHROMA_DB_DIR}...")
        # Use the list of line_documents directly
        vector_store = Chroma.from_documents(
            documents=documents, # Use the list of Document objects
            embedding=embeddings,
            collection_name="markdown_image_chunk_collection", # Changed collection name
            persist_directory=CHROMA_DB_DIR # Use the defined persist directory
        )
        logger.info("ƒê√£ t·∫°o v√† l∆∞u tr·ªØ Chroma vector store t·ª´ c√°c d√≤ng.")
        return vector_store
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o vector store t·ª´ c√°c d√≤ng: {e}")
        st.error("Kh√¥ng th·ªÉ t·∫°o vector store t·ª´ c√°c d√≤ng markdown. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh embeddings v√† l∆∞u tr·ªØ.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None


def main():
    """Main function to run the Streamlit application."""
    st.set_page_config(page_title="Tr√≤ chuy·ªán v·ªõi PDF", page_icon="üìö")
    st.title("üìò Tr√≤ chuy·ªán v·ªõi t√†i li·ªáu PDF c·ªßa b·∫°n.")
    init_session_state()

    st.sidebar.header("T·∫£i l√™n & T√πy ch·ªçn")
    # Using file uploader for PDF files
    uploaded_files = st.sidebar.file_uploader("T·∫£i l√™n t·ªáp PDF", type="pdf", accept_multiple_files=False) # Changed to False for single file processing

    # Poppler path is now fixed
    poppler_path = FIXED_POPPLER_PATH
    logger.info(f"S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n Poppler c·ªë ƒë·ªãnh: {poppler_path}")


    if st.sidebar.button("üîÑ ƒê·∫∑t l·∫°i Tr√≤ chuy·ªán"):
        reset_chat()
        st.rerun()

    process_button = st.sidebar.button("üöÄ X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n")

    # Process the uploaded PDF file when the button is clicked and a file is uploaded
    # Also process if a file is uploaded and no vector store is ready yet
    if uploaded_files and (process_button or not st.session_state.get('vector_store_ready')):

        # Check if a new file is uploaded or if we need to re-process the current one
        current_uploaded_file_name = uploaded_files.name if uploaded_files else None
        if current_uploaded_file_name != st.session_state.get('processed_pdf_name') or process_button or not st.session_state.get('vector_store_ready'):

            st.session_state.pop('chain', None)
            st.session_state.pop('vector_store', None)
            reset_chat() # Reset chat before processing new document

            st.session_state['processed_pdf_name'] = current_uploaded_file_name # Store the name of the currently processed PDF

            with st.spinner(f"ƒêang x·ª≠ l√Ω t·ªáp PDF: {uploaded_files.name}..."):
                temp_pdf_path = None
                try:
                    # Save the uploaded PDF to a temporary file
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                        tmp.write(uploaded_files.getvalue())
                        temp_pdf_path = tmp.name
                    logger.info(f"ƒê√£ l∆∞u t·ªáp PDF t·∫°m th·ªùi t·∫°i: {temp_pdf_path}")

                    # Call the get_md_text function to convert PDF to markdown
                    # This function is assumed to save the output to MARKDOWN_OUTPUT_FILE_PATH
                    logger.info(f"ƒêang chuy·ªÉn ƒë·ªïi PDF sang Markdown: {temp_pdf_path}")
                    markdown_text = get_md_text(temp_pdf_path, output_dir_base=MARKDOWN_OUTPUT_DIR, poppler_path=poppler_path)
                    logger.info(f"Qu√° tr√¨nh chuy·ªÉn ƒë·ªïi PDF sang Markdown ho√†n t·∫•t. K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i {MARKDOWN_OUTPUT_FILE_PATH}")

                    # Now read the generated markdown file and create chunks based on image lines
                    chunks = process_markdown_file_lines(MARKDOWN_OUTPUT_FILE_PATH)

                    if chunks:
                        vector_store = create_vector_store_from_documents(chunks)
                        if vector_store:
                            st.session_state['vector_store'] = vector_store
                            st.session_state['chain'] = create_chain(vector_store)
                            st.session_state['vector_store_ready'] = True # Set flag to True
                            st.success(f"T√†i li·ªáu PDF '{uploaded_files.name}' ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng! B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi.")
                        else:
                            st.error("Kh√¥ng th·ªÉ t·∫°o vector store t·ª´ c√°c chunks.")
                    else:
                         st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t ho·∫∑c chunked t·ª´ t·ªáp Markdown.")

                except Exception as e:
                    logger.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω PDF ho·∫∑c Markdown: {e}")
                    st.error(f"ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω t√†i li·ªáu. Vui l√≤ng th·ª≠ l·∫°i.")
                    st.error(f"L·ªói c·ª• th·ªÉ: {e}")
                finally:
                    # Clean up the temporary PDF file
                    if temp_pdf_path and os.path.exists(temp_pdf_path):
                        try:
                            os.unlink(temp_pdf_path)
                            logger.info(f"ƒê√£ x√≥a t·ªáp PDF t·∫°m th·ªùi: {temp_pdf_path}")
                        except OSError as e:
                            logger.error(f"L·ªói khi x√≥a t·ªáp PDF t·∫°m th·ªùi {temp_pdf_path}: {e}")
                    # Note: The generated markdown file and directory are cleaned up in reset_chat

    # Display chat interface if the chain is ready
    if st.session_state.get('chain') is not None and st.session_state.get('vector_store_ready'):
        display_chat(st.session_state["chain"])
    elif uploaded_files and not st.session_state.get('vector_store_ready'):
         st.info("ƒêang ch·ªù x·ª≠ l√Ω t·ªáp PDF. Vui l√≤ng nh·∫•n 'X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n'.")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF v√† nh·∫•n 'X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

if __name__ == "__main__":
    main()
