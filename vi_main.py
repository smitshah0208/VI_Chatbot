"""
Complete vi_main.py file with robust error handling for cross-environment compatibility
"""
import streamlit as st
from streamlit_chat import message
from langchain.chains import ConversationalRetrievalChain
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from dotenv import load_dotenv
import os
import logging
import tempfile
import shutil
import re
import sys
import uuid

# Configure logging to show more detailed information
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import PDF processing function
try:
    from process_pdf import get_md_text
    logger.info("ƒê√£ nh·∫≠p h√†m get_md_text t·ª´ process_pdf.py th√†nh c√¥ng.")
except ImportError:
    logger.error("Kh√¥ng t√¨m th·∫•y process_pdf.py ho·∫∑c h√†m get_md_text. Vui l√≤ng ƒë·∫£m b·∫£o t·ªáp t·ªìn t·∫°i.")
    
    # Define a dummy function to prevent errors if import fails
    def get_md_text(pdf_path, output_dir_base="md_output", poppler_path=None):
        logger.error("H√†m get_md_text kh√¥ng kh·∫£ d·ª•ng do l·ªói nh·∫≠p.")
        return ""

load_dotenv()

# Configuration constants
MARKDOWN_OUTPUT_FILENAME = "document.md"
MARKDOWN_OUTPUT_DIR = "md_output"
MARKDOWN_OUTPUT_FILE_PATH = os.path.join(MARKDOWN_OUTPUT_DIR, MARKDOWN_OUTPUT_FILENAME)

# Multiple potential DB directories to try
DB_DIRECTORIES = [
    "./md_chroma_db",                              # Original location
    os.path.join(tempfile.gettempdir(), "md_chroma_db"),  # Temp directory
    os.path.expanduser("~/md_chroma_db"),          # Home directory
    "/tmp/md_chroma_db"                            # Linux/Unix tmp directory
]

# Default to first option, but will be updated if needed
CHROMA_DB_DIR = DB_DIRECTORIES[0]

# Define the fixed Poppler path - adapt as needed for different environments
if sys.platform.startswith('win'):
    FIXED_POPPLER_PATH = r".\poppler-24.08.0\Library\bin"
else:
    FIXED_POPPLER_PATH = None  # Let system find it on Linux/Mac

# Regex to identify markdown image lines
IMAGE_LINE_REGEX = re.compile(r'^!\[.*?\]\(.*?\)$')


def init_session_state():
    """Initializes Streamlit session state variables."""
    st.session_state.setdefault('history', [])
    st.session_state.setdefault('generated', ["C√°c ph·∫£n h·ªìi c·ªßa b·∫°n s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã ·ªü ƒë√¢y."])
    st.session_state.setdefault('past', ["Xin ch√†o! üëã H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu PDF ƒë√£ t·∫£i l√™n üìÑ"])
    st.session_state.setdefault('processed_pdf_name', None)
    st.session_state.setdefault('vector_store_ready', False)
    st.session_state.setdefault('current_db_path', CHROMA_DB_DIR)
    st.session_state.setdefault('unique_session_id', str(uuid.uuid4()))


def safely_remove_path(path, is_dir=False):
    """Safely removes a file or directory with robust error handling."""
    if not os.path.exists(path):
        return True
        
    try:
        if is_dir:
            shutil.rmtree(path)
            logger.info(f"ƒê√£ x√≥a th∆∞ m·ª•c: {path}")
        else:
            os.remove(path)
            logger.info(f"ƒê√£ x√≥a t·ªáp: {path}")
        return True
    except (OSError, PermissionError) as e:
        logger.warning(f"Kh√¥ng th·ªÉ x√≥a {'th∆∞ m·ª•c' if is_dir else 't·ªáp'} {path}: {e}")
        
        # If we can't remove it, try renaming it with a unique identifier
        try:
            unique_suffix = st.session_state.get('unique_session_id', str(uuid.uuid4()))[:8]
            new_path = f"{path}_{unique_suffix}_old"
            os.rename(path, new_path)
            logger.info(f"Kh√¥ng th·ªÉ x√≥a, ƒë√£ ƒë·ªïi t√™n th√†nh: {new_path}")
            return True
        except Exception as rename_err:
            logger.error(f"C·∫£ x√≥a v√† ƒë·ªïi t√™n ƒë·ªÅu th·∫•t b·∫°i cho {path}: {rename_err}")
            return False


def reset_chat():
    """Resets the chat history and processing state with improved cleanup."""
    # Reset session state
    st.session_state['history'] = []
    st.session_state['past'] = ["Xin ch√†o! üëã H√£y h·ªèi t√¥i b·∫•t c·ª© ƒëi·ªÅu g√¨ v·ªÅ t√†i li·ªáu PDF ƒë√£ t·∫£i l√™n üìÑ"]
    st.session_state.pop('chain', None)
    st.session_state.pop('vector_store', None)
    st.session_state['processed_pdf_name'] = None
    st.session_state['vector_store_ready'] = False
    
    # Generate new session ID for unique filenames if needed
    st.session_state['unique_session_id'] = str(uuid.uuid4())
    
    # Clean up files with robust error handling
    safely_remove_path(MARKDOWN_OUTPUT_FILE_PATH, is_dir=False)
    safely_remove_path(MARKDOWN_OUTPUT_DIR, is_dir=True)
    
    # Clean up all potential DB directories
    for db_dir in DB_DIRECTORIES:
        if os.path.exists(db_dir):
            safely_remove_path(db_dir, is_dir=True)
    
    # Reset current DB path to default
    st.session_state['current_db_path'] = CHROMA_DB_DIR
    
    logger.info("Chat v√† tr·∫°ng th√°i x·ª≠ l√Ω ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t l·∫°i.")


def conversation_chat(query, chain, history):
    """Handles the conversational chat interaction."""
    try:
        logger.info(f"Nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng: {query}")
        result = chain.invoke({"question": query, "chat_history": history})
        answer = result["answer"].strip()
        history.append((query, answer))
        logger.info("ƒê√£ t·∫°o ph·∫£n h·ªìi th√†nh c√¥ng.")
        return answer
    except Exception as e:
        logger.error(f"L·ªói h·ªôi tho·∫°i: {e}")
        error_msg = f"Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω y√™u c·∫ßu n√†y ngay b√¢y gi·ªù. L·ªói: {e}"
        history.append((query, error_msg))
        return error_msg


def display_chat(chain):
    """Displays the chat interface."""
    with st.form("chat_form", clear_on_submit=True):
        user_input = st.text_input("C√¢u h·ªèi:", placeholder=f"H√£y h·ªèi v·ªÅ n·ªôi dung c·ªßa t√†i li·ªáu {st.session_state.get('processed_pdf_name', 'PDF')}", key='input')
        if st.form_submit_button("G·ª≠i") and user_input:
            with st.spinner("ƒêang t·∫°o ph·∫£n h·ªìi..."):
                output = conversation_chat(user_input, chain, st.session_state['history'])
                st.session_state['past'].append(user_input)
                st.session_state['generated'].append(output)
            # Rerun to update the chat display
            st.rerun()

    # Display chat messages
    if st.session_state['generated']:
        for i in reversed(range(len(st.session_state['generated']))):
            if i < len(st.session_state["past"]):
                message(st.session_state["past"][i], is_user=True, key=f"{i}_user")
            message(st.session_state["generated"][i], key=str(i))


def create_chain(vector_store):
    """Creates a conversational retrieval chain."""
    try:
        logger.info("ƒêang t·∫°o chu·ªói h·ªôi tho·∫°i...")
        llm = AzureChatOpenAI(
            deployment_name=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"),
            model_name=os.getenv("AZURE_OPENAI_CHAT_MODEL_NAME"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_version=os.getenv("OPENAI_API_CHAT_MODEL_VERSION"),
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            openai_api_type="azure",
            temperature=0
        )

        memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        # Adjust search_kwargs if needed, e.g., number of relevant documents to retrieve
        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})

        # Prompt for markdown context
        template = """S·ª≠ d·ª•ng c√°c ƒëo·∫°n vƒÉn b·∫£n sau t·ª´ t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Tr·∫£ l·ªùi **ch·ªâ b·∫±ng ti·∫øng Vi·ªát**.
N·∫øu b·∫°n kh√¥ng bi·∫øt c√¢u tr·∫£ l·ªùi, ch·ªâ c·∫ßn n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt, ƒë·ª´ng c·ªë t·∫°o ra c√¢u tr·∫£ l·ªùi.

Ng·ªØ c·∫£nh:
{context}

C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi:"""
        prompt = PromptTemplate(template=template, input_variables=["context", "question"])

        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            memory=memory,
            combine_docs_chain_kwargs={'prompt': prompt},
            return_source_documents=False
        )
        logger.info("ƒê√£ t·∫°o chu·ªói h·ªôi tho·∫°i th√†nh c√¥ng.")
        return chain
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o chu·ªói h·ªôi tho·∫°i: {e}")
        st.error("Kh√¥ng th·ªÉ t·∫°o chu·ªói h·ªôi tho·∫°i. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh Azure OpenAI.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None


def process_markdown_file_lines(markdown_file_path: str):
    """
    Reads a markdown file and creates document chunks based on content
    following image lines.
    """
    chunks = []
    current_chunk_lines = []
    if not os.path.exists(markdown_file_path):
        logger.error(f"L·ªói: T·ªáp markdown kh√¥ng t√¨m th·∫•y t·∫°i {markdown_file_path}")
        st.error(f"L·ªói: T·ªáp markdown kh√¥ng t√¨m th·∫•y t·∫°i {markdown_file_path}. Qu√° tr√¨nh chuy·ªÉn ƒë·ªïi PDF sang Markdown c√≥ th·ªÉ ƒë√£ th·∫•t b·∫°i ho·∫∑c t·ªáp kh√¥ng ƒë∆∞·ª£c l∆∞u ƒë√∫ng c√°ch.")
        return None

    try:
        logger.info(f"ƒêang ƒë·ªçc t·ªáp markdown v√† t·∫°o chunks d·ª±a tr√™n h√¨nh ·∫£nh: {markdown_file_path}")
        with open(markdown_file_path, 'r', encoding='utf-8') as f:
            # Read the file line by line
            for i, line in enumerate(f):
                cleaned_line = line.strip()

                # Check if the cleaned line is an image line
                if IMAGE_LINE_REGEX.match(cleaned_line):
                    # If we encounter an image line, the content accumulated so far
                    # (if any) forms a chunk.
                    if current_chunk_lines:
                        chunk_content = "\n".join(current_chunk_lines)
                        # Create a Document object for the chunk
                        chunks.append(Document(page_content=chunk_content, metadata={"source": os.path.basename(markdown_file_path), "start_line": i - len(current_chunk_lines) + 1}))
                        logger.debug(f"ƒê√£ t·∫°o chunk t·ª´ d√≤ng {i - len(current_chunk_lines) + 1} ƒë·∫øn {i-1}")

                    # Clear the buffer to start accumulating content for the next chunk
                    current_chunk_lines = []
                    # Do NOT add the image line itself to the current chunk buffer

                elif cleaned_line: # If it's not an image line and not an empty line
                    # Add the cleaned line to the current chunk buffer
                    current_chunk_lines.append(cleaned_line)

        # After the loop, add any remaining content in the buffer as the last chunk
        if current_chunk_lines:
            chunk_content = "\n".join(current_chunk_lines)
            chunks.append(Document(page_content=chunk_content, metadata={"source": os.path.basename(markdown_file_path), "start_line": i - len(current_chunk_lines) + 2}))
            logger.debug(f"ƒê√£ t·∫°o chunk cu·ªëi c√πng t·ª´ d√≤ng {i - len(current_chunk_lines) + 2} ƒë·∫øn {i+1}")

        logger.info(f"ƒê√£ t·∫°o {len(chunks)} chunks d·ª±a tr√™n h√¨nh ·∫£nh t·ª´ {markdown_file_path}")
        return chunks

    except Exception as e:
        logger.error(f"L·ªói khi ƒë·ªçc t·ªáp markdown v√† t·∫°o chunks {markdown_file_path}: {e}")
        st.error(f"Kh√¥ng th·ªÉ ƒë·ªçc t·ªáp markdown ho·∫∑c t·∫°o chunks: {markdown_file_path}. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng t·ªáp ho·∫∑c quy·ªÅn truy c·∫≠p.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None


def find_writable_directory():
    """Find a writable directory for the database from the list of potential directories."""
    for db_path in DB_DIRECTORIES:
        try:
            # Ensure the directory exists
            os.makedirs(db_path, exist_ok=True)
            
            # Test write permissions
            test_file = os.path.join(db_path, "test_write.txt")
            with open(test_file, 'w') as f:
                f.write("Testing write permissions")
            os.remove(test_file)
            
            logger.info(f"T√¨m th·∫•y th∆∞ m·ª•c c√≥ th·ªÉ ghi: {db_path}")
            return db_path
        except Exception as e:
            logger.warning(f"Kh√¥ng th·ªÉ s·ª≠ d·ª•ng th∆∞ m·ª•c {db_path}: {e}")
            continue
            
    # If no writable directory found, use temp directory with unique name
    fallback_dir = os.path.join(tempfile.gettempdir(), f"md_chroma_{st.session_state['unique_session_id']}")
    try:
        os.makedirs(fallback_dir, exist_ok=True)
        logger.info(f"S·ª≠ d·ª•ng th∆∞ m·ª•c t·∫°m th·ªùi duy nh·∫•t: {fallback_dir}")
        return fallback_dir
    except Exception as e:
        logger.error(f"Kh√¥ng th·ªÉ t·∫°o th∆∞ m·ª•c t·∫°m th·ªùi: {e}")
        return None


def create_vector_store_from_documents(documents: list[Document]):
    """
    Generates a Chroma vector store with robust error handling and fallback options.
    """
    if not documents:
        st.error("Kh√¥ng c√≥ t√†i li·ªáu n√†o ƒë·ªÉ t·∫°o vector store.")
        return None

    # Initialize embeddings
    try:
        logger.info("ƒêang kh·ªüi t·∫°o AzureOpenAIEmbeddings...")
        embeddings = AzureOpenAIEmbeddings(
            deployment=os.getenv("OPENAI_ADA_EMBEDDING_EMBEDDING_DEPLOYMENT_NAME"),
            model=os.getenv("OPENAI_ADA_EMBEDDING_MODEL_NAME"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            openai_api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            openai_api_type="azure",
            chunk_size=100
        )
        logger.info("ƒê√£ kh·ªüi t·∫°o AzureOpenAIEmbeddings th√†nh c√¥ng.")
    except Exception as e:
        logger.error(f"L·ªói khi kh·ªüi t·∫°o AzureOpenAIEmbeddings: {e}")
        st.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o Embeddings t·ª´ Azure OpenAI. Vui l√≤ng ki·ªÉm tra bi·∫øn m√¥i tr∆∞·ªùng.")
        st.error(f"L·ªói c·ª• th·ªÉ: {e}")
        return None

    # Find a writable directory
    db_path = find_writable_directory()
    if not db_path:
        logger.warning("Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c c√≥ th·ªÉ ghi, s·ª≠ d·ª•ng vector store trong b·ªô nh·ªõ.")
        try:
            # Create in-memory vector store as last resort
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=embeddings
            )
            logger.info("ƒê√£ t·∫°o vector store trong b·ªô nh·ªõ th√†nh c√¥ng")
            return vector_store
        except Exception as e:
            logger.critical(f"Kh√¥ng th·ªÉ t·∫°o vector store ngay c·∫£ trong b·ªô nh·ªõ: {e}")
            st.error("Kh√¥ng th·ªÉ t·∫°o vector store. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh.")
            st.error(f"L·ªói c·ª• th·ªÉ: {e}")
            return None
    
    # Store the current DB path in session state
    st.session_state['current_db_path'] = db_path
    
    # Create vector store in the writable directory
    try:
        logger.info(f"ƒêang t·∫°o Chroma vector store t·∫°i {db_path}...")
        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            collection_name="markdown_image_chunk_collection",
            persist_directory=db_path
        )
        logger.info(f"ƒê√£ t·∫°o th√†nh c√¥ng Chroma vector store t·∫°i: {db_path}")
        return vector_store
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o vector store t·∫°i {db_path}: {e}")
        
        # Try once more without persistence as fallback
        try:
            logger.info("Th·ª≠ t·∫°o vector store trong b·ªô nh·ªõ...")
            vector_store = Chroma.from_documents(
                documents=documents,
                embedding=embeddings
            )
            logger.info("ƒê√£ t·∫°o vector store trong b·ªô nh·ªõ th√†nh c√¥ng")
            return vector_store
        except Exception as e2:
            logger.critical(f"Kh√¥ng th·ªÉ t·∫°o vector store ngay c·∫£ trong b·ªô nh·ªõ: {e2}")
            st.error("Kh√¥ng th·ªÉ t·∫°o vector store. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh.")
            st.error(f"L·ªói c·ª• th·ªÉ: {e2}")
            return None


def main():
    """Main function to run the Streamlit application."""
    st.set_page_config(page_title="Tr√≤ chuy·ªán v·ªõi PDF", page_icon="üìö")
    st.title("üìò Tr√≤ chuy·ªán v·ªõi t√†i li·ªáu PDF c·ªßa b·∫°n.")
    init_session_state()

    st.sidebar.header("T·∫£i l√™n & T√πy ch·ªçn")
    uploaded_files = st.sidebar.file_uploader("T·∫£i l√™n t·ªáp PDF", type="pdf", accept_multiple_files=False)

    # Determine Poppler path based on platform
    poppler_path = FIXED_POPPLER_PATH
    logger.info(f"S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n Poppler: {poppler_path}")

    # Add a status indicator for the vector store
    if st.session_state.get('vector_store_ready'):
        st.sidebar.success(f"PDF ƒë√£ s·∫µn s√†ng: {st.session_state.get('processed_pdf_name', '')}")
        db_path = st.session_state.get('current_db_path', 'b·ªô nh·ªõ')
        st.sidebar.info(f"C∆° s·ªü d·ªØ li·ªáu: {db_path}")
    else:
        st.sidebar.warning("")

    if st.sidebar.button("üîÑ ƒê·∫∑t l·∫°i Tr√≤ chuy·ªán"):
        reset_chat()
        st.rerun()

    process_button = st.sidebar.button("üöÄ X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n")

    # Process the uploaded PDF file
    if uploaded_files and (process_button or not st.session_state.get('vector_store_ready')):
        current_uploaded_file_name = uploaded_files.name if uploaded_files else None
        
        if current_uploaded_file_name != st.session_state.get('processed_pdf_name') or process_button or not st.session_state.get('vector_store_ready'):
            st.session_state.pop('chain', None)
            st.session_state.pop('vector_store', None)
            reset_chat()  # Reset chat before processing new document

            st.session_state['processed_pdf_name'] = current_uploaded_file_name

            with st.spinner(f"ƒêang x·ª≠ l√Ω t·ªáp PDF: {uploaded_files.name}..."):
                temp_pdf_path = None
                try:
                    # Create the output directory if it doesn't exist
                    os.makedirs(MARKDOWN_OUTPUT_DIR, exist_ok=True)
                    
                    # Save the uploaded PDF to a temporary file
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
                        tmp.write(uploaded_files.getvalue())
                        temp_pdf_path = tmp.name
                    logger.info(f"ƒê√£ l∆∞u t·ªáp PDF t·∫°m th·ªùi t·∫°i: {temp_pdf_path}")

                    # Convert PDF to markdown
                    logger.info(f"ƒêang chuy·ªÉn ƒë·ªïi PDF sang Markdown: {temp_pdf_path}")
                    markdown_text = get_md_text(temp_pdf_path, output_dir_base=MARKDOWN_OUTPUT_DIR, poppler_path=poppler_path)
                    logger.info(f"Qu√° tr√¨nh chuy·ªÉn ƒë·ªïi PDF sang Markdown ho√†n t·∫•t. K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u t·∫°i {MARKDOWN_OUTPUT_FILE_PATH}")

                    # Create chunks based on image lines
                    chunks = process_markdown_file_lines(MARKDOWN_OUTPUT_FILE_PATH)

                    if chunks and len(chunks) > 0:
                        # Create vector store with robust error handling
                        vector_store = create_vector_store_from_documents(chunks)
                        
                        if vector_store:
                            st.session_state['vector_store'] = vector_store
                            st.session_state['chain'] = create_chain(vector_store)
                            st.session_state['vector_store_ready'] = True
                            st.success(f"T√†i li·ªáu PDF '{uploaded_files.name}' ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω th√†nh c√¥ng! B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ƒë·∫∑t c√¢u h·ªèi.")
                        else:
                            st.error("Kh√¥ng th·ªÉ t·∫°o vector store t·ª´ c√°c chunks.")
                    else:
                        st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t ho·∫∑c chunked t·ª´ t·ªáp Markdown.")

                except Exception as e:
                    logger.error(f"L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω PDF ho·∫∑c Markdown: {e}")
                    st.error("ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω t√†i li·ªáu. Vui l√≤ng th·ª≠ l·∫°i.")
                    st.error(f"L·ªói c·ª• th·ªÉ: {e}")
                finally:
                    # Clean up the temporary PDF file
                    if temp_pdf_path and os.path.exists(temp_pdf_path):
                        try:
                            os.unlink(temp_pdf_path)
                            logger.info(f"ƒê√£ x√≥a t·ªáp PDF t·∫°m th·ªùi: {temp_pdf_path}")
                        except OSError as e:
                            logger.warning(f"L·ªói khi x√≥a t·ªáp PDF t·∫°m th·ªùi {temp_pdf_path}: {e}")

    # Display chat interface if ready
    if st.session_state.get('chain') is not None and st.session_state.get('vector_store_ready'):
        display_chat(st.session_state["chain"])
    elif uploaded_files and not st.session_state.get('vector_store_ready'):
        st.info("ƒêang ch·ªù x·ª≠ l√Ω t·ªáp PDF. Vui l√≤ng nh·∫•n 'X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n'.")
    else:
        st.info("Vui l√≤ng t·∫£i l√™n t√†i li·ªáu PDF v√† nh·∫•n 'X·ª≠ l√Ω t·ªáp PDF ƒë√£ t·∫£i l√™n' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")


if __name__ == "__main__":
    main()